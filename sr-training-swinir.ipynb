{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cszn/KAIR.git"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Cloning into 'KAIR'...\nremote: Enumerating objects: 1773, done.\u001b[K\nremote: Total 1773 (delta 0), reused 0 (delta 0), pack-reused 1773\u001b[K\nReceiving objects: 100% (1773/1773), 19.38 MiB | 13.68 MiB/s, done.\nResolving deltas: 100% (1051/1051), done.\nUpdating files: 100% (279/279), done.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r KAIR/requirement.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: opencv-python in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 1)) (4.7.0.72)\nRequirement already satisfied: scikit-image in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 2)) (0.20.0)\nRequirement already satisfied: pillow in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 3)) (9.2.0)\nRequirement already satisfied: torchvision in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 4)) (0.9.1)\nRequirement already satisfied: hdf5storage in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 5)) (0.1.19)\nRequirement already satisfied: ninja in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 6)) (1.11.1)\nRequirement already satisfied: lmdb in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 7)) (1.4.1)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 8)) (2.28.2)\nRequirement already satisfied: timm in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 9)) (0.9.2)\nRequirement already satisfied: einops in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from -r KAIR/requirement.txt (line 10)) (0.6.1)\nRequirement already satisfied: numpy>=1.17.3; python_version >= \"3.8\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencv-python->-r KAIR/requirement.txt (line 1)) (1.18.5)\nRequirement already satisfied: imageio>=2.4.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (2.27.0)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (21.3)\nRequirement already satisfied: PyWavelets>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (1.4.1)\nRequirement already satisfied: scipy<1.9.2,>=1.8; python_version <= \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (1.8.0)\nRequirement already satisfied: lazy_loader>=0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (0.2)\nRequirement already satisfied: tifffile>=2019.7.26 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (2023.3.21)\nRequirement already satisfied: networkx>=2.8 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-image->-r KAIR/requirement.txt (line 2)) (3.1)\nRequirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from torchvision->-r KAIR/requirement.txt (line 4)) (1.12.0)\nRequirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from hdf5storage->-r KAIR/requirement.txt (line 5)) (2.10.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->-r KAIR/requirement.txt (line 8)) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->-r KAIR/requirement.txt (line 8)) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->-r KAIR/requirement.txt (line 8)) (3.0.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->-r KAIR/requirement.txt (line 8)) (1.26.14)\nRequirement already satisfied: safetensors in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from timm->-r KAIR/requirement.txt (line 9)) (0.3.1)\nRequirement already satisfied: huggingface-hub in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from timm->-r KAIR/requirement.txt (line 9)) (0.12.0)\nRequirement already satisfied: pyyaml in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from timm->-r KAIR/requirement.txt (line 9)) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from packaging>=20.0->scikit-image->-r KAIR/requirement.txt (line 2)) (3.0.9)\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from torch->torchvision->-r KAIR/requirement.txt (line 4)) (3.10.0.0)\nRequirement already satisfied: six in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage->-r KAIR/requirement.txt (line 5)) (1.16.0)\nRequirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from huggingface-hub->timm->-r KAIR/requirement.txt (line 9)) (4.64.1)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from huggingface-hub->timm->-r KAIR/requirement.txt (line 9)) (3.9.0)\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python KAIR/main_download_pretrained_models.py --models \"SwinIR\" --model_dir \"model_zoo\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python KAIR/main_train_psnr.py --opt KAIR/options/swinir/train_swinir_sr_realworld_x4_gan.json"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "export CUDA_VISIBLE_DEVICES=0\nnumber of GPUs is: 1\nLogHandlers setup!\n23-05-25 16:58:42.060 :   task: swinir_sr_realworld_x4_gan\n  model: gan\n  gpu_ids: [0]\n  scale: 4\n  n_channels: 3\n  path:[\n    root: KAIR/superresolution\n    pretrained_netG: KAIR/superresolution/swinir_sr_realworld_x4_gan/models/75000_G.pth\n    pretrained_netD: None\n    pretrained_netE: KAIR/superresolution/swinir_sr_realworld_x4_gan/models/75000_E.pth\n    task: KAIR/superresolution/swinir_sr_realworld_x4_gan\n    log: KAIR/superresolution/swinir_sr_realworld_x4_gan\n    options: KAIR/superresolution/swinir_sr_realworld_x4_gan/options\n    models: KAIR/superresolution/swinir_sr_realworld_x4_gan/models\n    images: KAIR/superresolution/swinir_sr_realworld_x4_gan/images\n    pretrained_optimizerG: KAIR/superresolution/swinir_sr_realworld_x4_gan/models/75000_optimizerG.pth\n  ]\n  datasets:[\n    train:[\n      name: train_dataset\n      dataset_type: blindsr\n      dataroot_H: datasets/FFHQ-SCUT/HR\n      dataroot_L: None\n      degradation_type: bsrgan\n      H_size: 256\n      shuffle_prob: 0.1\n      lq_patchsize: 64\n      use_sharp: True\n      dataloader_shuffle: True\n      dataloader_num_workers: 1\n      dataloader_batch_size: 1\n      phase: train\n      scale: 4\n      n_channels: 3\n    ]\n    test:[\n      name: test_dataset\n      dataset_type: blindsr\n      degradation_type: bsrgan\n      H_size: 256\n      shuffle_prob: 0.1\n      lq_patchsize: 64\n      use_sharp: False\n      dataroot_H: KAIR/testsets/Set5\n      dataroot_L: None\n      phase: test\n      scale: 4\n      n_channels: 3\n    ]\n  ]\n  netG:[\n    net_type: swinir\n    upscale: 4\n    in_chans: 3\n    img_size: 64\n    window_size: 8\n    img_range: 1.0\n    depths: [6, 6, 6, 6, 6, 6]\n    embed_dim: 180\n    num_heads: [6, 6, 6, 6, 6, 6]\n    mlp_ratio: 2\n    upsampler: nearest+conv\n    resi_connection: 1conv\n    init_type: default\n    scale: 4\n  ]\n  netD:[\n    net_type: discriminator_unet\n    in_nc: 3\n    base_nc: 64\n    n_layers: 3\n    norm_type: spectral\n    init_type: orthogonal\n    init_bn_type: uniform\n    init_gain: 0.2\n  ]\n  train:[\n    G_lossfn_type: l1\n    G_lossfn_weight: 1\n    F_lossfn_type: l1\n    F_lossfn_weight: 1\n    F_feature_layer: [2, 7, 16, 25, 34]\n    F_weights: [0.1, 0.1, 1.0, 1.0, 1.0]\n    F_use_input_norm: True\n    F_use_range_norm: False\n    gan_type: gan\n    D_lossfn_weight: 0.1\n    E_decay: 0.999\n    D_init_iters: 0\n    G_optimizer_type: adam\n    G_optimizer_lr: 0.0001\n    G_optimizer_wd: 0\n    D_optimizer_type: adam\n    D_optimizer_lr: 0.0001\n    D_optimizer_wd: 0\n    G_scheduler_type: MultiStepLR\n    G_scheduler_milestones: [10000, 50000, 80000, 100000]\n    G_scheduler_gamma: 0.5\n    G_optimizer_reuse: True\n    D_scheduler_type: MultiStepLR\n    D_scheduler_milestones: [10000, 50000, 80000, 100000]\n    D_scheduler_gamma: 0.5\n    D_optimizer_reuse: False\n    G_param_strict: True\n    D_param_strict: True\n    E_param_strict: True\n    checkpoint_test: 5000\n    checkpoint_save: 5000\n    checkpoint_print: 100\n    G_optimizer_betas: [0.9, 0.999]\n    G_scheduler_restart_weights: 1\n  ]\n  opt_path: KAIR/options/swinir/train_swinir_sr_realworld_x4_gan.json\n  is_train: True\n  merge_bn: False\n  merge_bn_startpoint: -1\n  find_unused_parameters: True\n  use_static_graph: False\n  dist: False\n  num_gpu: 1\n  rank: 0\n  world_size: 1\n\nRandom seed: 2496\n1100\nDataset [DatasetBlindSR - train_dataset] is created.\n23-05-25 16:58:42.456 : Number of train images: 1,100, iters: 1,100\n5\nDataset [DatasetBlindSR - test_dataset] is created.\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nPass this initialization! Initialization was done during network definition!\nusing the UNet discriminator\nInitialization method [orthogonal + uniform], gain is [0.20]\nPass this initialization! Initialization was done during network definition!\nTraining model [ModelGAN] is created.\nLoading model for G [KAIR/superresolution/swinir_sr_realworld_x4_gan/models/75000_G.pth] ...\nLoading model for E [KAIR/superresolution/swinir_sr_realworld_x4_gan/models/75000_E.pth] ...\nSequential(\n  (child0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (child1): Sequential(\n    (0): ReLU(inplace=True)\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (child2): Sequential(\n    (0): ReLU(inplace=True)\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (child3): Sequential(\n    (0): ReLU(inplace=True)\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (child4): Sequential(\n    (0): ReLU(inplace=True)\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n)\nfeature_layer: [2, 7, 16, 25, 34]  with weights: [0.1, 0.1, 1.0, 1.0, 1.0]\nLoading optimizerG [KAIR/superresolution/swinir_sr_realworld_x4_gan/models/75000_optimizerG.pth] ...\n23-05-25 16:58:54.259 : \nNetworks name: SwinIR\nParams number: 11715559\nNet structure:\nSwinIR(\n  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (patch_embed): PatchEmbed(\n    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n  )\n  (patch_unembed): PatchUnEmbed()\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (layers): ModuleList(\n    (0): RSTB(\n      (residual_group): BasicLayer(\n        dim=180, input_resolution=(64, 64), depth=6\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): Identity()\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.003)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.006)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.009)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.011)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.014)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (patch_embed): PatchEmbed()\n      (patch_unembed): PatchUnEmbed()\n    )\n    (1): RSTB(\n      (residual_group): BasicLayer(\n        dim=180, input_resolution=(64, 64), depth=6\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.017)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.020)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.023)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.026)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.029)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.031)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (patch_embed): PatchEmbed()\n      (patch_unembed): PatchUnEmbed()\n    )\n    (2): RSTB(\n      (residual_group): BasicLayer(\n        dim=180, input_resolution=(64, 64), depth=6\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.034)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.037)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.040)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.043)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.046)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.049)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (patch_embed): PatchEmbed()\n      (patch_unembed): PatchUnEmbed()\n    )\n    (3): RSTB(\n      (residual_group): BasicLayer(\n        dim=180, input_resolution=(64, 64), depth=6\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.051)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.054)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.057)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.060)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.063)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.066)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (patch_embed): PatchEmbed()\n      (patch_unembed): PatchUnEmbed()\n    )\n    (4): RSTB(\n      (residual_group): BasicLayer(\n        dim=180, input_resolution=(64, 64), depth=6\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.069)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.071)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.074)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.077)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.080)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.083)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (patch_embed): PatchEmbed()\n      (patch_unembed): PatchUnEmbed()\n    )\n    (5): RSTB(\n      (residual_group): BasicLayer(\n        dim=180, input_resolution=(64, 64), depth=6\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.086)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.089)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.091)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.094)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.097)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=180, window_size=(8, 8), num_heads=6\n              (qkv): Linear(in_features=180, out_features=540, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=180, out_features=180, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath(drop_prob=0.100)\n            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=180, out_features=360, bias=True)\n              (act): GELU(approximate=none)\n              (fc2): Linear(in_features=360, out_features=180, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (patch_embed): PatchEmbed()\n      (patch_unembed): PatchUnEmbed()\n    )\n  )\n  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv_before_upsample): Sequential(\n    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n  )\n  (conv_up1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv_up2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv_hr): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n)\n\nNetworks name: Discriminator_UNet\nParams number: 4376897\nNet structure:\nDiscriminator_UNet(\n  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n23-05-25 16:58:54.397 : \n |  mean  |  min   |  max   |  std   || shape               \n | -0.001 | -0.448 |  0.480 |  0.124 | torch.Size([180, 3, 3, 3]) || conv_first.weight\n |  0.007 | -0.839 |  0.453 |  0.158 | torch.Size([180]) || conv_first.bias\n |  1.168 |  0.518 |  2.435 |  0.262 | torch.Size([180]) || patch_embed.norm.weight\n | -0.009 | -1.304 |  0.406 |  0.137 | torch.Size([180]) || patch_embed.norm.bias\n |  0.393 | -0.004 |  1.078 |  0.178 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight\n | -0.005 | -0.588 |  0.757 |  0.219 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias\n | -0.270 | -2.115 |  1.980 |  0.676 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index\n |  0.001 | -1.634 |  1.719 |  0.165 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight\n | -0.042 | -1.264 |  1.289 |  0.415 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias\n |  0.000 | -1.125 |  0.876 |  0.071 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight\n | -0.008 | -1.239 |  0.234 |  0.122 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias\n |  0.527 |  0.000 |  1.078 |  0.183 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight\n | -0.022 | -0.547 |  0.496 |  0.207 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias\n |  0.000 | -1.062 |  0.859 |  0.101 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight\n | -0.034 | -0.269 |  0.250 |  0.055 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias\n | -0.001 | -1.042 |  1.407 |  0.101 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight\n |  0.006 | -1.357 |  0.526 |  0.133 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask\n |  0.537 |  0.012 |  1.189 |  0.191 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight\n | -0.014 | -0.700 |  0.908 |  0.201 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias\n | -0.158 | -2.929 |  1.623 |  0.480 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index\n | -0.000 | -1.138 |  0.976 |  0.167 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight\n |  0.015 | -0.919 |  0.917 |  0.289 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias\n |  0.000 | -0.737 |  0.712 |  0.094 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight\n |  0.003 | -0.950 |  0.242 |  0.118 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias\n |  0.597 |  0.314 |  1.261 |  0.166 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight\n | -0.018 | -0.375 |  0.549 |  0.150 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias\n |  0.001 | -0.922 |  0.972 |  0.114 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight\n | -0.043 | -0.201 |  0.112 |  0.047 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias\n |  0.000 | -0.804 |  1.729 |  0.087 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight\n |  0.005 | -0.782 |  0.227 |  0.084 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias\n |  0.671 |  0.111 |  1.104 |  0.170 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight\n | -0.026 | -0.912 |  1.221 |  0.227 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias\n | -0.136 | -1.786 |  1.909 |  0.516 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index\n |  0.000 | -2.034 |  2.061 |  0.173 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight\n |  0.014 | -0.847 |  0.826 |  0.221 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias\n |  0.001 | -0.677 |  0.527 |  0.112 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight\n | -0.003 | -0.504 |  0.296 |  0.112 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias\n |  0.692 |  0.175 |  1.141 |  0.151 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight\n | -0.025 | -0.400 |  0.772 |  0.142 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias\n |  0.001 | -0.968 |  0.853 |  0.128 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight\n | -0.040 | -0.233 |  0.087 |  0.046 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias\n | -0.000 | -1.023 |  0.596 |  0.084 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight\n | -0.002 | -0.422 |  0.164 |  0.078 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask\n |  0.717 |  0.125 |  1.054 |  0.171 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight\n | -0.036 | -0.843 |  1.437 |  0.210 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias\n | -0.140 | -1.858 |  1.765 |  0.550 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index\n |  0.000 | -2.915 |  3.093 |  0.184 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight\n |  0.002 | -1.098 |  1.180 |  0.234 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias\n |  0.000 | -0.607 |  0.567 |  0.118 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight\n | -0.009 | -0.552 |  0.374 |  0.131 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias\n |  0.727 |  0.431 |  1.113 |  0.123 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight\n | -0.034 | -0.455 |  0.569 |  0.118 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias\n |  0.000 | -1.011 |  1.415 |  0.147 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight\n | -0.038 | -0.205 |  0.080 |  0.044 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias\n | -0.000 | -0.616 |  0.512 |  0.088 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight\n | -0.003 | -0.335 |  0.154 |  0.064 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias\n |  0.780 |  0.097 |  1.080 |  0.188 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight\n | -0.051 | -0.751 |  1.142 |  0.174 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias\n | -0.209 | -3.063 |  1.751 |  0.600 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index\n |  0.000 | -1.890 |  1.626 |  0.190 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight\n |  0.007 | -0.891 |  0.847 |  0.167 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias\n |  0.000 | -0.608 |  0.624 |  0.119 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight\n | -0.010 | -0.590 |  0.758 |  0.133 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias\n |  0.726 |  0.406 |  1.088 |  0.114 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight\n | -0.050 | -0.416 |  0.361 |  0.096 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias\n |  0.000 | -0.993 |  0.921 |  0.142 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight\n | -0.029 | -0.226 |  0.069 |  0.041 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias\n |  0.000 | -0.694 |  0.671 |  0.086 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight\n | -0.009 | -0.300 |  0.548 |  0.068 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask\n |  0.711 |  0.095 |  1.020 |  0.168 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight\n | -0.055 | -0.516 |  1.071 |  0.150 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias\n | -0.227 | -2.293 |  2.396 |  0.762 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index\n |  0.001 | -0.984 |  0.928 |  0.187 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight\n | -0.004 | -0.716 |  0.777 |  0.137 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias\n | -0.001 | -1.082 |  1.036 |  0.115 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight\n | -0.014 | -0.284 |  1.216 |  0.134 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias\n |  0.502 |  0.248 |  0.984 |  0.085 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight\n | -0.047 | -0.336 |  0.463 |  0.073 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias\n |  0.001 | -0.945 |  0.932 |  0.130 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight\n | -0.046 | -0.167 |  0.091 |  0.042 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias\n | -0.000 | -1.437 |  1.120 |  0.091 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight\n | -0.010 | -0.132 |  1.003 |  0.087 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias\n |  0.000 | -0.685 |  0.734 |  0.099 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight\n | -0.000 | -0.876 |  0.848 |  0.220 | torch.Size([180]) || layers.0.conv.bias\n |  0.728 |  0.060 |  1.201 |  0.248 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight\n |  0.014 | -0.907 |  0.913 |  0.184 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias\n | -0.066 | -2.006 |  1.495 |  0.495 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index\n |  0.000 | -0.857 |  1.118 |  0.149 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight\n | -0.011 | -0.889 |  0.810 |  0.146 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias\n |  0.001 | -0.798 |  0.815 |  0.152 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight\n | -0.009 | -1.122 |  0.729 |  0.167 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias\n |  0.732 |  0.310 |  1.121 |  0.136 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight\n |  0.012 | -0.431 |  0.464 |  0.110 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias\n |  0.001 | -1.047 |  0.806 |  0.145 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight\n | -0.068 | -0.232 |  0.081 |  0.050 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias\n |  0.001 | -1.030 |  1.110 |  0.140 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight\n | -0.001 | -1.213 |  0.894 |  0.157 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask\n |  0.778 |  0.146 |  1.118 |  0.210 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight\n |  0.015 | -0.851 |  0.952 |  0.184 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias\n |  0.020 | -1.654 |  1.404 |  0.402 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index\n | -0.000 | -1.173 |  0.909 |  0.156 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight\n | -0.007 | -0.858 |  0.837 |  0.132 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias\n | -0.000 | -1.024 |  0.966 |  0.150 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight\n | -0.003 | -1.554 |  1.084 |  0.192 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias\n |  0.786 |  0.412 |  1.262 |  0.127 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight\n |  0.013 | -0.830 |  0.509 |  0.134 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias\n |  0.000 | -0.856 |  0.811 |  0.169 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight\n | -0.070 | -0.224 |  0.086 |  0.046 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias\n |  0.000 | -1.116 |  0.936 |  0.153 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight\n |  0.000 | -1.275 |  0.864 |  0.158 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias\n |  0.862 |  0.128 |  1.148 |  0.191 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight\n |  0.016 | -1.120 |  0.978 |  0.195 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias\n |  0.128 | -2.958 |  2.175 |  0.541 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index\n |  0.001 | -1.166 |  0.990 |  0.158 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight\n | -0.004 | -0.674 |  0.607 |  0.114 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias\n |  0.000 | -0.739 |  0.699 |  0.149 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight\n |  0.002 | -1.310 |  1.073 |  0.187 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias\n |  0.760 |  0.352 |  1.068 |  0.094 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight\n |  0.009 | -0.792 |  0.523 |  0.132 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias\n |  0.001 | -0.808 |  0.714 |  0.166 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight\n | -0.093 | -0.247 |  0.052 |  0.045 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias\n | -0.001 | -1.117 |  0.949 |  0.147 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight\n |  0.003 | -0.734 |  0.628 |  0.124 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask\n |  0.878 |  0.127 |  1.121 |  0.163 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight\n |  0.020 | -1.285 |  0.979 |  0.183 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias\n |  0.121 | -3.187 |  2.221 |  0.667 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index\n |  0.000 | -1.067 |  0.947 |  0.161 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight\n |  0.005 | -0.570 |  0.546 |  0.118 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias\n | -0.000 | -0.608 |  0.589 |  0.145 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight\n |  0.012 | -0.796 |  1.415 |  0.185 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias\n |  0.751 |  0.414 |  1.085 |  0.080 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight\n |  0.015 | -0.593 |  0.562 |  0.107 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias\n |  0.001 | -0.762 |  0.737 |  0.164 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight\n | -0.094 | -0.208 |  0.066 |  0.045 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias\n | -0.000 | -0.974 |  0.785 |  0.143 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight\n |  0.008 | -0.391 |  0.601 |  0.099 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias\n |  0.901 |  0.146 |  1.146 |  0.152 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight\n |  0.025 | -1.464 |  1.039 |  0.186 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias\n |  0.127 | -2.907 |  1.875 |  0.589 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index\n |  0.000 | -1.208 |  0.947 |  0.160 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight\n | -0.002 | -0.512 |  0.377 |  0.098 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias\n | -0.000 | -0.613 |  0.627 |  0.143 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight\n |  0.019 | -0.403 |  1.407 |  0.170 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias\n |  0.687 |  0.338 |  0.988 |  0.075 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight\n |  0.014 | -0.648 |  0.446 |  0.106 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias\n |  0.002 | -0.692 |  0.710 |  0.159 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight\n | -0.098 | -0.262 |  0.028 |  0.050 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias\n | -0.001 | -0.850 |  0.839 |  0.142 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight\n |  0.009 | -0.469 |  0.865 |  0.107 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask\n |  0.859 |  0.199 |  1.058 |  0.114 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight\n |  0.030 | -1.389 |  0.865 |  0.177 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias\n |  0.085 | -2.234 |  1.800 |  0.517 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index\n |  0.000 | -0.773 |  1.009 |  0.158 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight\n | -0.004 | -0.354 |  0.355 |  0.084 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias\n | -0.001 | -0.973 |  1.054 |  0.142 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight\n |  0.020 | -0.918 |  1.202 |  0.152 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias\n |  0.505 |  0.324 |  0.688 |  0.058 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight\n |  0.016 | -0.491 |  0.546 |  0.088 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias\n |  0.004 | -0.641 |  0.733 |  0.147 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight\n | -0.113 | -0.289 |  0.085 |  0.059 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias\n | -0.002 | -0.878 |  1.170 |  0.150 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight\n |  0.018 | -1.309 |  0.761 |  0.146 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias\n | -0.000 | -0.511 |  0.546 |  0.076 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight\n | -0.000 | -0.764 |  1.583 |  0.211 | torch.Size([180]) || layers.1.conv.bias\n |  0.744 |  0.124 |  1.024 |  0.181 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight\n |  0.016 | -1.125 |  0.624 |  0.161 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias\n |  0.015 | -3.115 |  1.248 |  0.456 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index\n |  0.000 | -0.744 |  0.756 |  0.148 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight\n |  0.004 | -0.558 |  0.717 |  0.114 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias\n | -0.000 | -0.715 |  0.726 |  0.170 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight\n |  0.000 | -0.665 |  0.783 |  0.156 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias\n |  0.982 |  0.369 |  1.599 |  0.149 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight\n |  0.026 | -0.631 |  0.397 |  0.135 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias\n |  0.000 | -1.050 |  1.132 |  0.166 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight\n | -0.109 | -0.289 |  0.074 |  0.062 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias\n | -0.000 | -0.944 |  1.066 |  0.175 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight\n |  0.004 | -0.418 |  0.908 |  0.114 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask\n |  0.803 |  0.173 |  1.096 |  0.169 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight\n |  0.020 | -1.361 |  0.648 |  0.176 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias\n | -0.021 | -2.935 |  1.488 |  0.639 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index\n |  0.000 | -0.812 |  0.749 |  0.151 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight\n | -0.001 | -0.490 |  0.876 |  0.116 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias\n | -0.001 | -0.875 |  0.737 |  0.175 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight\n |  0.005 | -0.836 |  1.006 |  0.168 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias\n |  0.949 |  0.335 |  1.330 |  0.128 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight\n |  0.024 | -0.674 |  0.629 |  0.159 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias\n |  0.002 | -0.967 |  1.282 |  0.165 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight\n | -0.089 | -0.243 |  0.062 |  0.051 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias\n |  0.000 | -1.033 |  1.063 |  0.175 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight\n |  0.004 | -0.508 |  1.153 |  0.134 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias\n |  0.859 |  0.154 |  1.112 |  0.163 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight\n |  0.027 | -1.445 |  0.721 |  0.185 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias\n |  0.078 | -2.050 |  1.457 |  0.468 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index\n |  0.000 | -0.815 |  0.970 |  0.154 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight\n | -0.007 | -0.697 |  0.635 |  0.111 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias\n | -0.000 | -0.881 |  1.072 |  0.176 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight\n |  0.010 | -0.752 |  1.335 |  0.187 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias\n |  0.924 |  0.346 |  1.178 |  0.132 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight\n |  0.039 | -0.797 |  0.701 |  0.165 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias\n |  0.001 | -0.962 |  0.672 |  0.164 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight\n | -0.094 | -0.296 |  0.022 |  0.051 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias\n | -0.000 | -0.807 |  0.977 |  0.173 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight\n |  0.005 | -0.703 |  1.282 |  0.149 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask\n |  0.866 |  0.123 |  1.109 |  0.160 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight\n |  0.036 | -1.123 |  0.657 |  0.168 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias\n |  0.101 | -3.809 |  1.797 |  0.649 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index\n | -0.000 | -0.813 |  0.762 |  0.156 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight\n | -0.002 | -0.486 |  0.700 |  0.122 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias\n |  0.001 | -0.852 |  1.121 |  0.171 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight\n |  0.007 | -1.088 |  1.446 |  0.198 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias\n |  0.880 |  0.350 |  1.146 |  0.119 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight\n |  0.045 | -0.816 |  0.721 |  0.149 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias\n |  0.002 | -0.774 |  0.694 |  0.161 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight\n | -0.104 | -0.286 |  0.052 |  0.053 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias\n |  0.000 | -0.877 |  1.078 |  0.170 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight\n |  0.000 | -1.199 |  1.173 |  0.163 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias\n |  0.865 |  0.104 |  1.153 |  0.146 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight\n |  0.044 | -1.079 |  0.713 |  0.163 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias\n |  0.110 | -3.424 |  1.852 |  0.631 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index\n |  0.001 | -1.071 |  0.716 |  0.156 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight\n | -0.003 | -0.543 |  0.394 |  0.113 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias\n |  0.000 | -0.958 |  1.007 |  0.172 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight\n |  0.007 | -1.644 |  1.105 |  0.200 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias\n |  0.813 |  0.378 |  1.033 |  0.101 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight\n |  0.040 | -0.834 |  0.744 |  0.152 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias\n |  0.002 | -0.743 |  0.834 |  0.158 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight\n | -0.108 | -0.327 |  0.028 |  0.058 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias\n | -0.000 | -1.176 |  1.034 |  0.168 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight\n |  0.000 | -1.770 |  0.632 |  0.181 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask\n |  0.786 |  0.170 |  0.971 |  0.111 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight\n |  0.045 | -0.963 |  0.657 |  0.155 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias\n | -0.038 | -2.693 |  1.569 |  0.758 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index\n | -0.000 | -0.758 |  0.838 |  0.153 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight\n | -0.003 | -0.427 |  0.423 |  0.095 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias\n | -0.001 | -1.659 |  1.864 |  0.170 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight\n |  0.004 | -2.095 |  0.616 |  0.200 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias\n |  0.659 |  0.390 |  0.855 |  0.075 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight\n |  0.018 | -1.460 |  0.548 |  0.160 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias\n |  0.000 | -0.603 |  0.662 |  0.149 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight\n | -0.134 | -0.366 |  0.039 |  0.067 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias\n | -0.001 | -2.078 |  0.894 |  0.162 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight\n |  0.001 | -1.882 |  0.197 |  0.176 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias\n | -0.000 | -0.526 |  0.591 |  0.067 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight\n |  0.005 | -1.275 |  1.279 |  0.216 | torch.Size([180]) || layers.2.conv.bias\n |  0.706 |  0.182 |  0.953 |  0.145 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight\n |  0.008 | -1.170 |  0.651 |  0.155 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias\n |  0.002 | -2.279 |  1.604 |  0.711 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index\n |  0.000 | -0.901 |  1.018 |  0.152 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight\n | -0.004 | -0.573 |  0.508 |  0.097 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias\n | -0.001 | -0.841 |  0.808 |  0.195 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight\n |  0.002 | -0.368 |  0.490 |  0.156 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias\n |  1.166 |  0.600 |  1.746 |  0.127 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight\n |  0.006 | -0.463 |  0.374 |  0.144 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias\n |  0.001 | -1.093 |  1.284 |  0.172 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight\n | -0.113 | -0.409 |  0.078 |  0.080 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias\n | -0.001 | -0.950 |  0.891 |  0.184 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight\n | -0.003 | -0.236 |  0.304 |  0.084 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask\n |  0.755 |  0.255 |  1.000 |  0.146 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight\n |  0.007 | -1.035 |  0.684 |  0.153 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias\n |  0.158 | -2.040 |  1.651 |  0.643 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index\n |  0.000 | -0.915 |  0.937 |  0.152 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight\n |  0.006 | -0.486 |  0.528 |  0.101 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias\n | -0.000 | -0.928 |  0.783 |  0.187 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight\n |  0.004 | -0.422 |  0.454 |  0.158 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias\n |  1.183 |  0.590 |  1.489 |  0.140 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight\n |  0.007 | -0.640 |  0.557 |  0.157 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias\n |  0.001 | -1.039 |  1.104 |  0.172 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight\n | -0.099 | -0.346 |  0.067 |  0.063 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias\n | -0.001 | -0.839 |  1.111 |  0.187 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight\n | -0.004 | -0.321 |  0.214 |  0.105 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias\n |  0.758 |  0.238 |  1.022 |  0.145 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight\n |  0.007 | -1.217 |  0.730 |  0.166 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias\n |  0.154 | -1.976 |  1.503 |  0.590 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index\n |  0.000 | -0.862 |  0.899 |  0.154 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight\n | -0.001 | -0.559 |  0.380 |  0.093 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias\n | -0.000 | -0.782 |  0.916 |  0.190 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight\n |  0.013 | -0.520 |  0.502 |  0.167 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias\n |  1.211 |  0.664 |  1.573 |  0.140 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight\n |  0.005 | -0.502 |  0.528 |  0.127 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias\n |  0.001 | -0.875 |  0.998 |  0.174 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight\n | -0.097 | -0.267 |  0.106 |  0.059 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias\n | -0.001 | -0.834 |  0.871 |  0.193 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight\n | -0.000 | -0.310 |  0.348 |  0.116 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask\n |  0.783 |  0.248 |  1.074 |  0.160 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight\n |  0.007 | -1.283 |  0.693 |  0.166 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias\n |  0.177 | -2.206 |  2.434 |  0.659 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index\n |  0.000 | -0.778 |  0.848 |  0.154 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight\n |  0.004 | -0.563 |  0.425 |  0.107 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias\n | -0.000 | -0.797 |  1.045 |  0.186 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight\n |  0.007 | -0.497 |  0.597 |  0.162 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias\n |  1.201 |  0.667 |  1.620 |  0.157 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight\n |  0.006 | -0.445 |  0.552 |  0.146 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias\n |  0.001 | -0.907 |  0.996 |  0.176 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight\n | -0.098 | -0.259 |  0.039 |  0.054 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias\n | -0.000 | -0.917 |  0.958 |  0.199 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight\n |  0.000 | -0.374 |  0.468 |  0.130 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias\n |  0.789 |  0.234 |  1.053 |  0.156 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight\n |  0.005 | -1.614 |  0.857 |  0.182 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias\n |  0.041 | -4.546 |  2.089 |  0.724 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index\n |  0.000 | -0.882 |  0.909 |  0.155 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight\n | -0.002 | -0.541 |  0.476 |  0.112 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias\n | -0.000 | -1.102 |  1.110 |  0.194 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight\n |  0.011 | -0.686 |  0.841 |  0.171 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias\n |  1.186 |  0.727 |  1.528 |  0.137 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight\n |  0.020 | -0.269 |  0.597 |  0.117 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias\n | -0.001 | -0.817 |  0.882 |  0.176 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight\n | -0.097 | -0.260 |  0.062 |  0.057 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias\n |  0.000 | -1.068 |  1.165 |  0.205 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight\n |  0.007 | -0.620 |  0.646 |  0.144 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask\n |  0.741 |  0.213 |  0.993 |  0.141 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight\n |  0.008 | -1.121 |  0.759 |  0.149 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias\n |  0.168 | -2.821 |  2.094 |  0.468 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index\n |  0.000 | -0.843 |  0.784 |  0.152 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight\n |  0.004 | -0.445 |  0.349 |  0.110 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias\n | -0.002 | -1.040 |  0.915 |  0.189 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight\n |  0.019 | -0.963 |  0.919 |  0.187 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias\n |  1.142 |  0.818 |  1.431 |  0.110 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight\n |  0.005 | -0.351 |  0.642 |  0.121 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias\n |  0.000 | -0.898 |  0.818 |  0.173 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight\n | -0.108 | -0.318 |  0.065 |  0.062 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias\n |  0.002 | -1.138 |  1.034 |  0.200 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight\n |  0.001 | -1.182 |  0.642 |  0.181 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias\n | -0.000 | -0.724 |  0.651 |  0.076 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight\n |  0.010 | -0.808 |  1.070 |  0.212 | torch.Size([180]) || layers.3.conv.bias\n |  0.838 |  0.257 |  1.226 |  0.186 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight\n | -0.010 | -0.648 |  0.448 |  0.125 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias\n |  0.148 | -4.444 |  1.570 |  0.588 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index\n | -0.000 | -1.330 |  1.377 |  0.177 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight\n |  0.001 | -0.603 |  0.552 |  0.113 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias\n |  0.000 | -1.219 |  1.130 |  0.249 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight\n |  0.008 | -0.481 |  0.533 |  0.194 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias\n |  1.672 |  0.997 |  1.974 |  0.160 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight\n | -0.006 | -0.591 |  0.594 |  0.244 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias\n | -0.001 | -1.096 |  0.923 |  0.204 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight\n | -0.188 | -0.684 |  0.194 |  0.117 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias\n | -0.000 | -0.917 |  0.997 |  0.213 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight\n |  0.005 | -0.184 |  0.217 |  0.072 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask\n |  0.896 |  0.252 |  1.298 |  0.213 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight\n | -0.009 | -0.673 |  0.617 |  0.107 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias\n |  0.111 | -4.860 |  2.652 |  0.809 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index\n | -0.000 | -1.140 |  1.268 |  0.174 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight\n | -0.007 | -0.579 |  0.562 |  0.126 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias\n |  0.001 | -1.194 |  1.006 |  0.254 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight\n |  0.002 | -0.453 |  0.557 |  0.220 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias\n |  1.698 |  1.076 |  1.991 |  0.167 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight\n |  0.019 | -0.578 |  0.578 |  0.219 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias\n | -0.005 | -0.894 |  1.249 |  0.204 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight\n | -0.157 | -0.519 |  0.126 |  0.107 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias\n | -0.001 | -1.008 |  1.030 |  0.222 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight\n |  0.005 | -0.225 |  0.262 |  0.097 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias\n |  0.864 |  0.267 |  1.228 |  0.209 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight\n | -0.008 | -0.506 |  0.633 |  0.091 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias\n |  0.175 | -4.106 |  1.867 |  0.587 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index\n |  0.000 | -1.302 |  1.085 |  0.173 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight\n |  0.003 | -0.602 |  0.496 |  0.121 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias\n | -0.001 | -1.190 |  1.114 |  0.270 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight\n |  0.004 | -0.601 |  0.556 |  0.201 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias\n |  1.771 |  1.187 |  2.091 |  0.160 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight\n | -0.005 | -0.485 |  0.439 |  0.179 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias\n | -0.003 | -0.920 |  1.239 |  0.205 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight\n | -0.141 | -0.426 |  0.155 |  0.091 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias\n | -0.000 | -1.546 |  1.279 |  0.237 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight\n |  0.007 | -0.251 |  0.266 |  0.100 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask\n |  0.852 |  0.282 |  1.341 |  0.213 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight\n | -0.010 | -0.478 |  0.717 |  0.100 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias\n |  0.151 | -3.380 |  1.368 |  0.432 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index\n | -0.000 | -1.194 |  1.141 |  0.168 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight\n |  0.005 | -0.500 |  0.595 |  0.107 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias\n |  0.000 | -1.203 |  1.176 |  0.261 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight\n |  0.001 | -0.475 |  0.416 |  0.192 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias\n |  1.752 |  1.171 |  2.066 |  0.166 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight\n | -0.002 | -0.434 |  0.615 |  0.202 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias\n | -0.004 | -1.014 |  0.841 |  0.203 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight\n | -0.139 | -0.417 |  0.079 |  0.083 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias\n | -0.000 | -1.217 |  1.298 |  0.242 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight\n |  0.002 | -0.295 |  0.352 |  0.106 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias\n |  0.857 |  0.271 |  1.391 |  0.218 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight\n | -0.011 | -0.581 |  0.779 |  0.111 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias\n |  0.026 | -5.440 |  2.136 |  0.624 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index\n | -0.000 | -1.606 |  1.117 |  0.166 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight\n | -0.002 | -0.568 |  0.407 |  0.099 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias\n |  0.001 | -1.461 |  1.224 |  0.269 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight\n | -0.008 | -0.506 |  0.482 |  0.188 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias\n |  1.709 |  1.037 |  2.129 |  0.164 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight\n |  0.004 | -0.681 |  0.590 |  0.221 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias\n | -0.004 | -0.900 |  0.859 |  0.202 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight\n | -0.141 | -0.503 |  0.078 |  0.081 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias\n |  0.000 | -1.445 |  1.357 |  0.258 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight\n |  0.003 | -0.262 |  0.337 |  0.092 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask\n |  0.840 |  0.311 |  1.238 |  0.197 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight\n | -0.017 | -0.608 |  0.635 |  0.103 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias\n |  0.014 | -2.939 |  1.442 |  0.529 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index\n |  0.000 | -1.047 |  1.020 |  0.165 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight\n | -0.003 | -0.716 |  0.459 |  0.101 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias\n |  0.001 | -1.406 |  1.239 |  0.262 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight\n | -0.008 | -0.530 |  0.589 |  0.201 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias\n |  1.734 |  1.123 |  2.080 |  0.155 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight\n |  0.006 | -0.699 |  0.664 |  0.243 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias\n | -0.005 | -0.969 |  0.989 |  0.205 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight\n | -0.138 | -0.416 |  0.077 |  0.080 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias\n | -0.000 | -1.694 |  1.691 |  0.275 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight\n | -0.002 | -0.314 |  0.355 |  0.114 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias\n | -0.000 | -0.715 |  0.727 |  0.105 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight\n |  0.027 | -1.341 |  0.706 |  0.288 | torch.Size([180]) || layers.4.conv.bias\n |  1.132 |  0.386 |  1.902 |  0.290 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight\n | -0.009 | -0.667 |  1.050 |  0.192 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias\n | -0.001 | -2.209 |  1.889 |  0.519 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index\n |  0.000 | -1.939 |  1.909 |  0.230 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight\n |  0.009 | -0.365 |  0.568 |  0.111 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias\n |  0.001 | -1.791 |  2.087 |  0.337 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight\n |  0.019 | -0.678 |  0.704 |  0.239 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias\n |  2.116 |  1.181 |  2.883 |  0.304 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight\n | -0.002 | -0.350 |  0.403 |  0.121 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias\n | -0.001 | -1.296 |  1.303 |  0.264 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight\n | -0.067 | -0.641 |  0.556 |  0.133 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias\n |  0.001 | -1.702 |  1.625 |  0.281 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight\n | -0.006 | -0.247 |  0.196 |  0.091 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask\n |  1.076 |  0.457 |  1.895 |  0.287 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight\n | -0.021 | -0.486 |  0.507 |  0.145 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias\n |  0.058 | -2.548 |  1.508 |  0.491 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index\n | -0.000 | -1.840 |  1.904 |  0.215 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight\n | -0.004 | -0.439 |  0.646 |  0.117 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias\n | -0.000 | -2.069 |  1.881 |  0.314 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight\n |  0.018 | -0.797 |  0.659 |  0.249 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias\n |  2.201 |  1.223 |  2.861 |  0.318 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight\n | -0.007 | -0.296 |  0.260 |  0.112 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias\n | -0.001 | -1.384 |  1.442 |  0.264 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight\n | -0.051 | -0.468 |  0.273 |  0.123 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias\n |  0.002 | -2.452 |  1.884 |  0.313 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight\n | -0.004 | -0.243 |  0.269 |  0.095 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias\n |  1.049 |  0.398 |  1.995 |  0.315 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight\n | -0.014 | -0.539 |  0.699 |  0.177 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias\n | -0.014 | -2.426 |  1.759 |  0.462 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index\n | -0.000 | -1.815 |  1.564 |  0.202 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight\n |  0.002 | -0.377 |  0.353 |  0.099 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias\n | -0.001 | -2.609 |  2.261 |  0.373 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight\n |  0.011 | -0.929 |  0.722 |  0.267 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias\n |  2.211 |  1.221 |  2.953 |  0.341 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight\n | -0.018 | -0.355 |  0.319 |  0.119 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias\n |  0.000 | -1.304 |  1.160 |  0.259 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight\n | -0.042 | -0.545 |  0.224 |  0.125 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias\n |  0.001 | -2.525 |  2.891 |  0.345 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight\n | -0.004 | -0.353 |  0.239 |  0.083 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask\n |  1.107 |  0.458 |  1.910 |  0.334 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight\n | -0.020 | -0.450 |  0.597 |  0.179 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias\n | -0.088 | -2.690 |  1.591 |  0.489 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index\n | -0.000 | -1.907 |  2.116 |  0.205 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight\n | -0.007 | -0.501 |  0.364 |  0.105 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias\n | -0.001 | -2.191 |  1.934 |  0.365 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight\n |  0.020 | -0.749 |  0.688 |  0.268 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias\n |  2.218 |  1.314 |  3.032 |  0.354 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight\n | -0.015 | -0.394 |  0.271 |  0.131 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias\n |  0.000 | -1.195 |  1.234 |  0.254 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight\n | -0.013 | -0.413 |  0.273 |  0.128 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias\n |  0.001 | -2.737 |  2.917 |  0.369 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight\n | -0.012 | -0.328 |  0.203 |  0.090 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias\n |  1.022 |  0.448 |  1.617 |  0.288 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight\n | -0.018 | -0.497 |  0.567 |  0.195 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias\n | -0.062 | -2.816 |  1.185 |  0.404 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index\n |  0.001 | -2.009 |  1.895 |  0.187 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight\n |  0.001 | -0.325 |  0.387 |  0.097 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias\n |  0.004 | -2.577 |  2.486 |  0.381 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight\n |  0.009 | -0.916 |  0.735 |  0.278 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias\n |  2.064 |  1.176 |  2.843 |  0.335 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight\n | -0.009 | -0.439 |  0.356 |  0.138 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias\n | -0.000 | -1.211 |  1.281 |  0.247 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight\n | -0.032 | -0.417 |  0.257 |  0.119 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias\n | -0.001 | -2.536 |  2.697 |  0.383 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight\n | -0.006 | -0.394 |  0.331 |  0.121 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias\n | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask\n |  1.066 |  0.431 |  1.702 |  0.289 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight\n | -0.007 | -0.532 |  0.741 |  0.214 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias\n | -0.022 | -1.406 |  1.073 |  0.283 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table\n | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index\n | -0.000 | -1.812 |  1.375 |  0.187 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight\n |  0.004 | -0.350 |  0.443 |  0.107 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias\n |  0.002 | -2.171 |  2.051 |  0.352 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight\n |  0.015 | -0.810 |  0.899 |  0.313 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias\n |  1.949 |  1.144 |  2.879 |  0.336 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight\n | -0.012 | -0.422 |  0.476 |  0.148 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias\n | -0.000 | -1.157 |  1.211 |  0.250 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight\n | -0.013 | -0.398 |  0.227 |  0.119 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias\n | -0.004 | -2.962 |  2.489 |  0.418 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight\n | -0.013 | -0.488 |  0.377 |  0.149 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias\n |  0.001 | -1.298 |  1.398 |  0.193 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight\n |  0.018 | -0.572 |  0.392 |  0.158 | torch.Size([180]) || layers.5.conv.bias\n |  0.069 |  0.018 |  0.344 |  0.050 | torch.Size([180]) || norm.weight\n | -0.003 | -0.119 |  0.547 |  0.062 | torch.Size([180]) || norm.bias\n | -0.000 | -0.741 |  0.706 |  0.138 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight\n | -0.009 | -0.368 |  0.650 |  0.131 | torch.Size([180]) || conv_after_body.bias\n |  0.000 | -0.478 |  1.024 |  0.085 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight\n | -0.085 | -0.645 |  0.089 |  0.176 | torch.Size([64]) || conv_before_upsample.0.bias\n | -0.005 | -0.632 |  0.538 |  0.079 | torch.Size([64, 64, 3, 3]) || conv_up1.weight\n | -0.036 | -0.145 |  0.104 |  0.052 | torch.Size([64]) || conv_up1.bias\n | -0.004 | -0.584 |  0.647 |  0.076 | torch.Size([64, 64, 3, 3]) || conv_up2.weight\n | -0.020 | -0.741 |  0.063 |  0.098 | torch.Size([64]) || conv_up2.bias\n | -0.006 | -0.793 |  0.800 |  0.091 | torch.Size([64, 64, 3, 3]) || conv_hr.weight\n |  0.033 | -0.114 |  0.164 |  0.048 | torch.Size([64]) || conv_hr.bias\n |  0.001 | -0.175 |  0.521 |  0.052 | torch.Size([3, 64, 3, 3]) || conv_last.weight\n | -0.093 | -0.109 | -0.073 |  0.019 | torch.Size([3]) || conv_last.bias\n\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\nTraceback (most recent call last):\n  File \"KAIR/main_train_psnr.py\", line 251, in <module>\n    main()\n  File \"KAIR/main_train_psnr.py\", line 188, in main\n    model.optimize_parameters(current_step)\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/mnguyen121/code/Users/mnguyen12/sr-training/KAIR/models/model_gan.py\", line 233, in optimize_parameters\n    self.G_optimizer.step()\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n    return wrapped(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 109, in wrapper\n    return func(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/adam.py\", line 157, in step\n    adam(params_with_grad,\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/adam.py\", line 213, in adam\n    func(params,\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/optim/adam.py\", line 255, in _single_tensor_adam\n    assert not step_t.is_cuda, \"If capturable=False, state_steps should not be CUDA tensors.\"\nAssertionError: If capturable=False, state_steps should not be CUDA tensors.\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}